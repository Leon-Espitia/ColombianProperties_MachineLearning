{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train dataset\n",
    "df_train = pd.read_csv(\"./properties_colombia_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand all dataset columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Expand the whole float numbers and leave them all with 2 decimals\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Henry's requirement\n",
    "Priorly, add the 'target' column from the 'price' feature <br>\n",
    "Null values will be replaced by its mean value<br>\n",
    "Then, set it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up categories values as 'expensive' and 'cheap' under the following statement rules\n",
    "price_avg = df_train['price'].mean()\n",
    "df_train['target'] = ['expensive' if price >= price_avg else 'cheap' for price in df_train['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dataset after adding the target column\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switchig to numerical values in the recent column added\n",
    "df_train['target'] = (df_train['target']== 'expensive').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and count 'target' column values \n",
    "df_train.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop 'price' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the 'price' column from df_train dataset\n",
    "df_train= df_train.drop(columns='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "Visual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking  numerical column and their metrics\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dataset general info\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking correlation of price and another numerical columns \n",
    "df_train = df_train[['Unnamed: 0','id', 'ad_type', 'start_date', 'end_date', 'created_on',\n",
    "       'lat', 'lon', 'l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'rooms', 'bedrooms',\n",
    "       'bathrooms', 'surface_total', 'surface_covered', 'currency',\n",
    "       'price_period', 'title', 'description', 'property_type',\n",
    "       'operation_type', 'geometry','target']]\n",
    "                       \n",
    "corr = df_train.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns,\n",
    "        annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous Person's correlation figure I´ve temporary choosen to keep 'lat', 'lon', 'bathrooms' numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop numerical features from Pearson´s analysis\n",
    "df_train = df_train.drop(columns={'Unnamed: 0','id', 'ad_type', 'start_date', 'end_date', 'created_on',\n",
    "       'rooms', 'bedrooms', 'surface_total', 'surface_covered'})\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After visualization analysis, I´ve decided to drop the following features:<br>\n",
    "a. 'l1' due to the whole properties being located in Colombia.<br>\n",
    "b. 'currency' and 'price_period' because they are irrelevant for properties' price clssification.<br>\n",
    "c. 'title' and 'description' because I won´t implement any \"Natural Language\" Model.<br>\n",
    "d. 'operation_type' all the dataset registers are about properties sale.<br>\n",
    "e. 'geometry' due to I going to use 'lat' and 'lon' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns irrelevant under my criterion\n",
    "df_train = df_train.drop(columns={'l1', 'currency', 'title', 'price_period', 'description', 'operation_type', 'geometry'})\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphically visualize missing feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python library to visualise missing values\n",
    "import missingno as msno\n",
    "\n",
    "msno.bar(df_train, figsize=(10,5), color='lightblue')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values porcentage in 'l4' column\n",
    "(df_train.l4.isnull().sum()/len(df_train)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous graphical analysis and supported by the percentage of the missing values decided to drop columns 'l4','l5','l6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "df_train = df_train.drop(columns={'l4', 'l5', 'l6'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features chose to fed the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change feature names\n",
    "df_train = df_train.rename(columns={'l2': 'department',\n",
    "                                    'l3': 'city', \n",
    "                                    'lat': 'latitude',\n",
    "                                    'lon': 'longitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking graphically\n",
    "msno.bar(df_train, figsize=(8,4), color='pink')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the null numerical feature values with their means\n",
    "df_train['latitude'] = df_train['latitude'].fillna(df_train['latitude'].mean())\n",
    "df_train['longitude'] = df_train['longitude'].fillna(df_train['longitude'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the null 'bathroom' values with their median\n",
    "df_train['bathrooms'] = df_train['bathrooms'].fillna(df_train['bathrooms'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'city' feature has some different column values from the same in the test dataset\n",
    "df_train = df_train.drop('city', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in the whole dataset\n",
    "df_train.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I´m going to use OneHotEncoder to work with categorical feature values and<br>\n",
    "apply it for each categorical feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "# Applay encoder to the feature\n",
    "codif = ohe.fit_transform(df_train[['department']])\n",
    "\n",
    "# Create a new DF with new values\n",
    "new_cols = pd.DataFrame(codif.toarray(), columns=ohe.categories_[0])\n",
    "\n",
    "# Join dataframes\n",
    "df_train = pd.concat([df_train, new_cols], axis=1)\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encoder to 'property_type' feature\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "# Applay encoder to the feature\n",
    "codif = ohe.fit_transform(df_train[['property_type']])\n",
    "\n",
    "# Create a new DF with new values\n",
    "new_cols = pd.DataFrame(codif.toarray(), columns=ohe.categories_[0])\n",
    "\n",
    "# Join dataframes\n",
    "df_train = pd.concat([df_train, new_cols], axis=1)\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dataframe\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'target' feature\n",
    "target = df_train.pop('target')\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete categorical features\n",
    "df_train = df_train.drop(columns=['department', 'property_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign variables to train the ML model\n",
    "X = df_train\n",
    "y = target"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
